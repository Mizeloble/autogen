{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Group Chat\n",
    "\n",
    "AutoGen offers conversable agents powered by LLM, tool or human, which can be used to perform tasks collectively via automated chat. This framework allows tool use and human participation through multi-agent conversation.\n",
    "Please find documentation about this feature [here](https://microsoft.github.io/autogen/docs/Use-Cases/agent_chat).\n",
    "\n",
    "This notebook is modified based on https://github.com/microsoft/FLAML/blob/4ea686af5c3e8ff24d9076a7a626c8b28ab5b1d7/notebook/autogen_multiagent_roleplay_chat.ipynb\n",
    "\n",
    "````{=mdx}\n",
    ":::info Requirements\n",
    "Install `pyautogen`:\n",
    "```bash\n",
    "pip install pyautogen\n",
    "```\n",
    "\n",
    "For more information, please refer to the [installation guide](/docs/installation/).\n",
    ":::\n",
    "````"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set your API Endpoint\n",
    "\n",
    "The [`config_list_from_json`](https://microsoft.github.io/autogen/docs/reference/oai/openai_utils#config_list_from_json) function loads a list of configurations from an environment variable or a json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import autogen\n",
    "\n",
    "config_list = autogen.config_list_from_json(\n",
    "    \"OAI_CONFIG_LIST\",\n",
    "    filter_dict={\n",
    "        \"model\": [\"gpt-4\", \"gpt-4-0314\", \"gpt4\", \"gpt-4-32k\", \"gpt-4-32k-0314\", \"gpt-4-32k-v0314\"],\n",
    "    },\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "````{=mdx}\n",
    ":::tip\n",
    "Learn more about configuring LLMs for agents [here](/docs/topics/llm_configuration).\n",
    ":::\n",
    "````\n",
    "\n",
    "## Construct Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_config = {\"config_list\": config_list, \"cache_seed\": 42}\n",
    "user_proxy = autogen.UserProxyAgent(\n",
    "    name=\"User_proxy\",\n",
    "    system_message=\"A human admin.\",\n",
    "    code_execution_config={\n",
    "        \"last_n_messages\": 2,\n",
    "        \"work_dir\": \"groupchat\",\n",
    "        \"use_docker\": False,\n",
    "    },  # Please set use_docker=True if docker is available to run the generated code. Using docker is safer than running the generated code directly.\n",
    "    human_input_mode=\"TERMINATE\",\n",
    ")\n",
    "coder = autogen.AssistantAgent(\n",
    "    name=\"Coder\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "pm = autogen.AssistantAgent(\n",
    "    name=\"Product_manager\",\n",
    "    system_message=\"Creative in software product ideas.\",\n",
    "    llm_config=llm_config,\n",
    ")\n",
    "groupchat = autogen.GroupChat(agents=[user_proxy, coder, pm], messages=[], max_round=12)\n",
    "manager = autogen.GroupChatManager(groupchat=groupchat, llm_config=llm_config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "To complete this task, I will need to perform two main steps:\n",
      "\n",
      "Step 1: Use the ArXiv API to search for the latest papers related to \"gpt-4\". ArXiv is known for hosting research papers from various fields and its API provides a search functionality where we can set the order to be recent first. We will parse the data and pick the first result which is the latest paper.\n",
      "\n",
      "Here is the Python code to perform this step:\n",
      "\n",
      "```python\n",
      "# Python code\n",
      "import urllib.request\n",
      "import feedparser\n",
      "import bs4 as bs\n",
      "import urllib.parse\n",
      "import re\n",
      "\n",
      "# Base api query url\n",
      "base_url = 'http://export.arxiv.org/api/query?'\n",
      "\n",
      "# Search parameters\n",
      "query = 'gpt-4'\n",
      "max_results = 1\n",
      "sortBy = 'submittedDate'\n",
      "sortOrder = 'descending'\n",
      "\n",
      "query = urllib.parse.urlencode({\n",
      "    'search_query': 'all:' + query,\n",
      "    'start': 0,\n",
      "    'max_results': max_results,\n",
      "    'sortBy': sortBy,\n",
      "    'sortOrder': sortOrder\n",
      "})\n",
      "\n",
      "# perform a GET request using the base_url and query\n",
      "response = urllib.request.urlopen(base_url+query).read()\n",
      "\n",
      "# parse the response using feedparser\n",
      "feed = feedparser.parse(response)\n",
      "\n",
      "if len(feed.entries) == 0:\n",
      "    print(\"No papers found!\")\n",
      "else:\n",
      "    # print out the paper information\n",
      "    for entry in feed.entries:\n",
      "        print('Paper title:   ', entry.title)\n",
      "        print('Paper author(s): ', ', '.join(author.name for author in entry.authors))\n",
      "        print('Paper published: ', entry.published)\n",
      "        print('Paper summary: ', entry.summary)\n",
      "        print('Paper link: ', entry.link)\n",
      "```\n",
      "\n",
      "Please run the above Python code to get the latest paper about \"gpt-4\" on arXiv.\n",
      "\n",
      "Step 2: After obtaining the paper, I will provide a Python script to scan and extract information from the paper. By analyzing the text, I will try to find instances where potential applications in software are mentioned.\n",
      "\n",
      "Here is the Python code to perform this step once you have the paper URL. Please replace 'paper_url' with the link retrieved from the previous step, and then execute the code.\n",
      "\n",
      "```python\n",
      "# Python code\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "import re\n",
      "\n",
      "def extract_content(url):\n",
      "    response = requests.get(url)\n",
      "    html = response.content\n",
      "    soup = BeautifulSoup(html, \"html.parser\")\n",
      "    text = soup.get_text()\n",
      "    return text\n",
      "\n",
      "def find_applications(text, keywords):\n",
      "    lines = text.split('\\n')\n",
      "    applications = []\n",
      "    for line in lines:\n",
      "        if any(keyword in line for keyword in keywords):\n",
      "            applications.append(line.strip())\n",
      "    return applications\n",
      "\n",
      "paper_url = ''  # Replace this with the link retrieved from previous step\n",
      "keywords = ['software', 'application', 'development', 'implementation', 'coding', 'programming']\n",
      "\n",
      "paper_content = extract_content(paper_url)\n",
      "applications = find_applications(paper_content, keywords)\n",
      "\n",
      "for application in applications:\n",
      "    print(application)\n",
      "```\n",
      "\n",
      "Please execute the Python code replacing `paper_url` with the actual link to download the paper. If the paper is in pdf format, this step may not work.\n",
      "\n",
      "Please note that these codes are simple text scan and formally fetching data from arXiv. Not all papers have clear mentions of applications, especially in the context of something as specific as software. Therefore, the code may not yield significant or clear results in that case. You may need to manually read the paper for comprehensive information. Please execute the code now.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Traceback (most recent call last):\n",
      "  File \"\", line 3, in <module>\n",
      "    import feedparser\n",
      "ModuleNotFoundError: No module named 'feedparser'\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "It seems like the 'feedparser' module is missing in your Python environment. This module is required to parse the RSS feed returned by the ArXiv API. You can install it using pip, which is a package manager for Python.\n",
      "\n",
      "Please execute the following command in your shell so that the required module is installed.\n",
      "\n",
      "```sh\n",
      "pip install feedparser\n",
      "```\n",
      "\n",
      "After you have successfully installed feedparser, please try running the previous Python code again.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is sh)...\u001b[0m\n",
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Requirement already satisfied: feedparser in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (6.0.11)\n",
      "Requirement already satisfied: sgmllib3k in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from feedparser) (1.0.0)\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is sh)...\u001b[0m\n",
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Requirement already satisfied: feedparser in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (6.0.11)\n",
      "Requirement already satisfied: sgmllib3k in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from feedparser) (1.0.0)\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "Great! Now that feedparser is installed, you can re-run the initial python code to fetch the latest paper related to 'gpt-4' from arXiv.\n",
      "\n",
      "Here's the code again:\n",
      "\n",
      "```python\n",
      "# Python code\n",
      "import urllib.request\n",
      "import feedparser\n",
      "import bs4 as bs\n",
      "import urllib.parse\n",
      "import re\n",
      "\n",
      "# Base api query url\n",
      "base_url = 'http://export.arxiv.org/api/query?'\n",
      "\n",
      "# Search parameters\n",
      "query = 'gpt-4'\n",
      "max_results = 1\n",
      "sortBy = 'submittedDate'\n",
      "sortOrder = 'descending'\n",
      "\n",
      "query = urllib.parse.urlencode({\n",
      "    'search_query': 'all:' + query,\n",
      "    'start': 0,\n",
      "    'max_results': max_results,\n",
      "    'sortBy': sortBy,\n",
      "    'sortOrder': sortOrder\n",
      "})\n",
      "\n",
      "# perform a GET request using the base_url and query\n",
      "response = urllib.request.urlopen(base_url+query).read()\n",
      "\n",
      "# parse the response using feedparser\n",
      "feed = feedparser.parse(response)\n",
      "\n",
      "if len(feed.entries) == 0:\n",
      "    print(\"No papers found!\")\n",
      "else:\n",
      "    # print out the paper information\n",
      "    for entry in feed.entries:\n",
      "        print('Paper title:   ', entry.title)\n",
      "        print('Paper author(s): ', ', '.join(author.name for author in entry.authors))\n",
      "        print('Paper published: ', entry.published)\n",
      "        print('Paper summary: ', entry.summary)\n",
      "        print('Paper link: ', entry.link)\n",
      "```\n",
      "\n",
      "Please run the above code.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 1 (execution failed)\n",
      "Code output: \n",
      "Traceback (most recent call last):\n",
      "  File \"\", line 3, in <module>\n",
      "    import feedparser\n",
      "ModuleNotFoundError: No module named 'feedparser'\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "I apologize for the confusion. It looks like the Python environment where the code is being executed might not be same as where the 'feedparser' module is installed. \n",
      "\n",
      "However, Python also provides an inbuilt module called xml for parsing XML data. The ArXiv API returns the data in Atom (which is an XML language) format which can be parsed and processed using Python's xml module.\n",
      "\n",
      "Let's modify the original code to use Python's xml and requests inbuilt modules instead of using external feedparser. Here is the modified Python code:\n",
      "\n",
      "```python\n",
      "# Python code\n",
      "import requests\n",
      "from xml.etree import ElementTree as ET\n",
      "\n",
      "# Base api query url\n",
      "base_url = 'http://export.arxiv.org/api/query?'\n",
      "\n",
      "# Search parameters\n",
      "query = 'gpt-4'\n",
      "max_results = 1\n",
      "sortBy = 'submittedDate'\n",
      "sortOrder = 'descending'\n",
      "\n",
      "query = {\n",
      "    'search_query': 'all:' + query,\n",
      "    'start': 0,\n",
      "    'max_results': max_results,\n",
      "    'sortBy': sortBy,\n",
      "    'sortOrder': sortOrder\n",
      "}\n",
      "\n",
      "# perform a GET request using the base_url and query\n",
      "response = requests.get(base_url, params = query)\n",
      "\n",
      "# parse the response using feedparser\n",
      "root = ET.fromstring(response.content)\n",
      "\n",
      "if root is None:\n",
      "    print(\"No papers found!\")\n",
      "else:\n",
      "    # print out the paper information\n",
      "    for entry in root.findall('{http://www.w3.org/2005/Atom}entry'):\n",
      "        print('Paper title:   ', entry.find('{http://www.w3.org/2005/Atom}title').text)\n",
      "        print('Paper author(s): ', ', '.join(author.find('{http://www.w3.org/2005/Atom}name').text for author in entry.findall('{http://www.w3.org/2005/Atom}author')))\n",
      "        print('Paper summary: ', entry.find('{http://www.w3.org/2005/Atom}summary').text)\n",
      "        print('Paper link: ', entry.find('{http://www.w3.org/2005/Atom}id').text)\n",
      "```\n",
      "\n",
      "Please run the modified Python code to fetch the latest paper related to \"gpt-4\" from arXiv.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[31m\n",
      ">>>>>>>> USING AUTO REPLY...\u001b[0m\n",
      "\u001b[31m\n",
      ">>>>>>>> EXECUTING CODE BLOCK 0 (inferred language is python)...\u001b[0m\n",
      "\u001b[33mUser_proxy\u001b[0m (to chat_manager):\n",
      "\n",
      "exitcode: 0 (execution succeeded)\n",
      "Code output: \n",
      "Paper title:    Keypoint Action Tokens Enable In-Context Imitation Learning in Robotics\n",
      "Paper author(s):  Norman Di Palo, Edward Johns\n",
      "Paper summary:    We show that off-the-shelf text-based Transformers, with no additional\n",
      "training, can perform few-shot in-context visual imitation learning, mapping\n",
      "visual observations to action sequences that emulate the demonstrator's\n",
      "behaviour. We achieve this by transforming visual observations (inputs) and\n",
      "trajectories of actions (outputs) into sequences of tokens that a\n",
      "text-pretrained Transformer (GPT-4 Turbo) can ingest and generate, via a\n",
      "framework we call Keypoint Action Tokens (KAT). Despite being trained only on\n",
      "language, we show that these Transformers excel at translating tokenised visual\n",
      "keypoint observations into action trajectories, performing on par or better\n",
      "than state-of-the-art imitation learning (diffusion policies) in the low-data\n",
      "regime on a suite of real-world, everyday tasks. Rather than operating in the\n",
      "language domain as is typical, KAT leverages text-based Transformers to operate\n",
      "in the vision and action domains to learn general patterns in demonstration\n",
      "data for highly efficient imitation learning, indicating promising new avenues\n",
      "for repurposing natural language models for embodied tasks. Videos are\n",
      "available at https://www.robot-learning.uk/keypoint-action-tokens.\n",
      "\n",
      "Paper link:  http://arxiv.org/abs/2403.19578v1\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\u001b[33mCoder\u001b[0m (to chat_manager):\n",
      "\n",
      "Based on the output provided, the latest paper titled \"Keypoint Action Tokens Enable In-Context Imitation Learning in Robotics\" by Norman Di Palo, Edward Johns focuses on how Transformers, including GPT-4 Turbo, can perform few-shot in-context visual imitation learning. \n",
      "\n",
      "This paper discusses repurposing natural language models for embodied tasks, including utilizing Transformers in the vision and action domains to learn general patterns in demonstration data for highly efficient imitation learning.\n",
      "\n",
      "Now, I'll provide a Python script to pull the paper text from the arXiv link. However, please note that because arXiv papers are typically in PDF format, this script might not be able to successfully extract the text. If the script fails, we recommend manually downloading and reading the PDF for information specific to software applications.\n",
      "\n",
      "Let's execute the following Python code:\n",
      "\n",
      "```python\n",
      "# Python code\n",
      "import requests\n",
      "from bs4 import BeautifulSoup\n",
      "import re\n",
      "\n",
      "def extract_content(url):\n",
      "    response = requests.get(url)\n",
      "    html = response.content\n",
      "    soup = BeautifulSoup(html, \"html.parser\")\n",
      "    text = soup.get_text()\n",
      "    return text\n",
      "\n",
      "def find_applications(text, keywords):\n",
      "    lines = text.split('\\n')\n",
      "    applications = []\n",
      "    for line in lines:\n",
      "        if any(keyword in line for keyword in keywords):\n",
      "            applications.append(line.strip())\n",
      "    return applications\n",
      "\n",
      "paper_url = 'http://arxiv.org/abs/2403.19578v1'  # Paper link\n",
      "keywords = ['software', 'application', 'development', 'implementation', 'coding', 'programming']\n",
      "\n",
      "paper_content = extract_content(paper_url)\n",
      "applications = find_applications(paper_content, keywords)\n",
      "\n",
      "for application in applications:\n",
      "    print(application)\n",
      "```\n",
      "\n",
      "This script attempts to scrape the paper content from the provided URL, then search the text for lines containing the keywords we're interested in (software-related terms). It then prints any matching lines.\n",
      "\n",
      "Please run this script. If it's not able to extract valuable information, you may need to manually read the paper by clicking [here](http://arxiv.org/abs/2403.19578v1).\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "ChatResult(chat_id=None, chat_history=[{'content': 'Find a latest paper about gpt-4 on arxiv and find its potential applications in software.', 'role': 'assistant'}, {'content': '', 'role': 'assistant'}, {'content': 'To complete this task, I will need to perform two main steps:\\n\\nStep 1: Use the ArXiv API to search for the latest papers related to \"gpt-4\". ArXiv is known for hosting research papers from various fields and its API provides a search functionality where we can set the order to be recent first. We will parse the data and pick the first result which is the latest paper.\\n\\nHere is the Python code to perform this step:\\n\\n```python\\n# Python code\\nimport urllib.request\\nimport feedparser\\nimport bs4 as bs\\nimport urllib.parse\\nimport re\\n\\n# Base api query url\\nbase_url = \\'http://export.arxiv.org/api/query?\\'\\n\\n# Search parameters\\nquery = \\'gpt-4\\'\\nmax_results = 1\\nsortBy = \\'submittedDate\\'\\nsortOrder = \\'descending\\'\\n\\nquery = urllib.parse.urlencode({\\n    \\'search_query\\': \\'all:\\' + query,\\n    \\'start\\': 0,\\n    \\'max_results\\': max_results,\\n    \\'sortBy\\': sortBy,\\n    \\'sortOrder\\': sortOrder\\n})\\n\\n# perform a GET request using the base_url and query\\nresponse = urllib.request.urlopen(base_url+query).read()\\n\\n# parse the response using feedparser\\nfeed = feedparser.parse(response)\\n\\nif len(feed.entries) == 0:\\n    print(\"No papers found!\")\\nelse:\\n    # print out the paper information\\n    for entry in feed.entries:\\n        print(\\'Paper title:   \\', entry.title)\\n        print(\\'Paper author(s): \\', \\', \\'.join(author.name for author in entry.authors))\\n        print(\\'Paper published: \\', entry.published)\\n        print(\\'Paper summary: \\', entry.summary)\\n        print(\\'Paper link: \\', entry.link)\\n```\\n\\nPlease run the above Python code to get the latest paper about \"gpt-4\" on arXiv.\\n\\nStep 2: After obtaining the paper, I will provide a Python script to scan and extract information from the paper. By analyzing the text, I will try to find instances where potential applications in software are mentioned.\\n\\nHere is the Python code to perform this step once you have the paper URL. Please replace \\'paper_url\\' with the link retrieved from the previous step, and then execute the code.\\n\\n```python\\n# Python code\\nimport requests\\nfrom bs4 import BeautifulSoup\\nimport re\\n\\ndef extract_content(url):\\n    response = requests.get(url)\\n    html = response.content\\n    soup = BeautifulSoup(html, \"html.parser\")\\n    text = soup.get_text()\\n    return text\\n\\ndef find_applications(text, keywords):\\n    lines = text.split(\\'\\\\n\\')\\n    applications = []\\n    for line in lines:\\n        if any(keyword in line for keyword in keywords):\\n            applications.append(line.strip())\\n    return applications\\n\\npaper_url = \\'\\'  # Replace this with the link retrieved from previous step\\nkeywords = [\\'software\\', \\'application\\', \\'development\\', \\'implementation\\', \\'coding\\', \\'programming\\']\\n\\npaper_content = extract_content(paper_url)\\napplications = find_applications(paper_content, keywords)\\n\\nfor application in applications:\\n    print(application)\\n```\\n\\nPlease execute the Python code replacing `paper_url` with the actual link to download the paper. If the paper is in pdf format, this step may not work.\\n\\nPlease note that these codes are simple text scan and formally fetching data from arXiv. Not all papers have clear mentions of applications, especially in the context of something as specific as software. Therefore, the code may not yield significant or clear results in that case. You may need to manually read the paper for comprehensive information. Please execute the code now.', 'name': 'Coder', 'role': 'user'}, {'content': 'exitcode: 1 (execution failed)\\nCode output: \\nTraceback (most recent call last):\\n  File \"\", line 3, in <module>\\n    import feedparser\\nModuleNotFoundError: No module named \\'feedparser\\'\\n', 'role': 'assistant'}, {'content': \"It seems like the 'feedparser' module is missing in your Python environment. This module is required to parse the RSS feed returned by the ArXiv API. You can install it using pip, which is a package manager for Python.\\n\\nPlease execute the following command in your shell so that the required module is installed.\\n\\n```sh\\npip install feedparser\\n```\\n\\nAfter you have successfully installed feedparser, please try running the previous Python code again.\", 'name': 'Coder', 'role': 'user'}, {'content': 'exitcode: 0 (execution succeeded)\\nCode output: \\nRequirement already satisfied: feedparser in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (6.0.11)\\nRequirement already satisfied: sgmllib3k in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from feedparser) (1.0.0)\\n', 'role': 'assistant'}, {'content': 'exitcode: 0 (execution succeeded)\\nCode output: \\nRequirement already satisfied: feedparser in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (6.0.11)\\nRequirement already satisfied: sgmllib3k in /anaconda/envs/azureml_py38/lib/python3.8/site-packages (from feedparser) (1.0.0)\\n', 'role': 'assistant'}, {'content': 'Great! Now that feedparser is installed, you can re-run the initial python code to fetch the latest paper related to \\'gpt-4\\' from arXiv.\\n\\nHere\\'s the code again:\\n\\n```python\\n# Python code\\nimport urllib.request\\nimport feedparser\\nimport bs4 as bs\\nimport urllib.parse\\nimport re\\n\\n# Base api query url\\nbase_url = \\'http://export.arxiv.org/api/query?\\'\\n\\n# Search parameters\\nquery = \\'gpt-4\\'\\nmax_results = 1\\nsortBy = \\'submittedDate\\'\\nsortOrder = \\'descending\\'\\n\\nquery = urllib.parse.urlencode({\\n    \\'search_query\\': \\'all:\\' + query,\\n    \\'start\\': 0,\\n    \\'max_results\\': max_results,\\n    \\'sortBy\\': sortBy,\\n    \\'sortOrder\\': sortOrder\\n})\\n\\n# perform a GET request using the base_url and query\\nresponse = urllib.request.urlopen(base_url+query).read()\\n\\n# parse the response using feedparser\\nfeed = feedparser.parse(response)\\n\\nif len(feed.entries) == 0:\\n    print(\"No papers found!\")\\nelse:\\n    # print out the paper information\\n    for entry in feed.entries:\\n        print(\\'Paper title:   \\', entry.title)\\n        print(\\'Paper author(s): \\', \\', \\'.join(author.name for author in entry.authors))\\n        print(\\'Paper published: \\', entry.published)\\n        print(\\'Paper summary: \\', entry.summary)\\n        print(\\'Paper link: \\', entry.link)\\n```\\n\\nPlease run the above code.', 'name': 'Coder', 'role': 'user'}, {'content': 'exitcode: 1 (execution failed)\\nCode output: \\nTraceback (most recent call last):\\n  File \"\", line 3, in <module>\\n    import feedparser\\nModuleNotFoundError: No module named \\'feedparser\\'\\n', 'role': 'assistant'}, {'content': 'I apologize for the confusion. It looks like the Python environment where the code is being executed might not be same as where the \\'feedparser\\' module is installed. \\n\\nHowever, Python also provides an inbuilt module called xml for parsing XML data. The ArXiv API returns the data in Atom (which is an XML language) format which can be parsed and processed using Python\\'s xml module.\\n\\nLet\\'s modify the original code to use Python\\'s xml and requests inbuilt modules instead of using external feedparser. Here is the modified Python code:\\n\\n```python\\n# Python code\\nimport requests\\nfrom xml.etree import ElementTree as ET\\n\\n# Base api query url\\nbase_url = \\'http://export.arxiv.org/api/query?\\'\\n\\n# Search parameters\\nquery = \\'gpt-4\\'\\nmax_results = 1\\nsortBy = \\'submittedDate\\'\\nsortOrder = \\'descending\\'\\n\\nquery = {\\n    \\'search_query\\': \\'all:\\' + query,\\n    \\'start\\': 0,\\n    \\'max_results\\': max_results,\\n    \\'sortBy\\': sortBy,\\n    \\'sortOrder\\': sortOrder\\n}\\n\\n# perform a GET request using the base_url and query\\nresponse = requests.get(base_url, params = query)\\n\\n# parse the response using feedparser\\nroot = ET.fromstring(response.content)\\n\\nif root is None:\\n    print(\"No papers found!\")\\nelse:\\n    # print out the paper information\\n    for entry in root.findall(\\'{http://www.w3.org/2005/Atom}entry\\'):\\n        print(\\'Paper title:   \\', entry.find(\\'{http://www.w3.org/2005/Atom}title\\').text)\\n        print(\\'Paper author(s): \\', \\', \\'.join(author.find(\\'{http://www.w3.org/2005/Atom}name\\').text for author in entry.findall(\\'{http://www.w3.org/2005/Atom}author\\')))\\n        print(\\'Paper summary: \\', entry.find(\\'{http://www.w3.org/2005/Atom}summary\\').text)\\n        print(\\'Paper link: \\', entry.find(\\'{http://www.w3.org/2005/Atom}id\\').text)\\n```\\n\\nPlease run the modified Python code to fetch the latest paper related to \"gpt-4\" from arXiv.', 'name': 'Coder', 'role': 'user'}, {'content': \"exitcode: 0 (execution succeeded)\\nCode output: \\nPaper title:    Keypoint Action Tokens Enable In-Context Imitation Learning in Robotics\\nPaper author(s):  Norman Di Palo, Edward Johns\\nPaper summary:    We show that off-the-shelf text-based Transformers, with no additional\\ntraining, can perform few-shot in-context visual imitation learning, mapping\\nvisual observations to action sequences that emulate the demonstrator's\\nbehaviour. We achieve this by transforming visual observations (inputs) and\\ntrajectories of actions (outputs) into sequences of tokens that a\\ntext-pretrained Transformer (GPT-4 Turbo) can ingest and generate, via a\\nframework we call Keypoint Action Tokens (KAT). Despite being trained only on\\nlanguage, we show that these Transformers excel at translating tokenised visual\\nkeypoint observations into action trajectories, performing on par or better\\nthan state-of-the-art imitation learning (diffusion policies) in the low-data\\nregime on a suite of real-world, everyday tasks. Rather than operating in the\\nlanguage domain as is typical, KAT leverages text-based Transformers to operate\\nin the vision and action domains to learn general patterns in demonstration\\ndata for highly efficient imitation learning, indicating promising new avenues\\nfor repurposing natural language models for embodied tasks. Videos are\\navailable at https://www.robot-learning.uk/keypoint-action-tokens.\\n\\nPaper link:  http://arxiv.org/abs/2403.19578v1\\n\", 'role': 'assistant'}, {'content': 'Based on the output provided, the latest paper titled \"Keypoint Action Tokens Enable In-Context Imitation Learning in Robotics\" by Norman Di Palo, Edward Johns focuses on how Transformers, including GPT-4 Turbo, can perform few-shot in-context visual imitation learning. \\n\\nThis paper discusses repurposing natural language models for embodied tasks, including utilizing Transformers in the vision and action domains to learn general patterns in demonstration data for highly efficient imitation learning.\\n\\nNow, I\\'ll provide a Python script to pull the paper text from the arXiv link. However, please note that because arXiv papers are typically in PDF format, this script might not be able to successfully extract the text. If the script fails, we recommend manually downloading and reading the PDF for information specific to software applications.\\n\\nLet\\'s execute the following Python code:\\n\\n```python\\n# Python code\\nimport requests\\nfrom bs4 import BeautifulSoup\\nimport re\\n\\ndef extract_content(url):\\n    response = requests.get(url)\\n    html = response.content\\n    soup = BeautifulSoup(html, \"html.parser\")\\n    text = soup.get_text()\\n    return text\\n\\ndef find_applications(text, keywords):\\n    lines = text.split(\\'\\\\n\\')\\n    applications = []\\n    for line in lines:\\n        if any(keyword in line for keyword in keywords):\\n            applications.append(line.strip())\\n    return applications\\n\\npaper_url = \\'http://arxiv.org/abs/2403.19578v1\\'  # Paper link\\nkeywords = [\\'software\\', \\'application\\', \\'development\\', \\'implementation\\', \\'coding\\', \\'programming\\']\\n\\npaper_content = extract_content(paper_url)\\napplications = find_applications(paper_content, keywords)\\n\\nfor application in applications:\\n    print(application)\\n```\\n\\nThis script attempts to scrape the paper content from the provided URL, then search the text for lines containing the keywords we\\'re interested in (software-related terms). It then prints any matching lines.\\n\\nPlease run this script. If it\\'s not able to extract valuable information, you may need to manually read the paper by clicking [here](http://arxiv.org/abs/2403.19578v1).', 'name': 'Coder', 'role': 'user'}], summary='Based on the output provided, the latest paper titled \"Keypoint Action Tokens Enable In-Context Imitation Learning in Robotics\" by Norman Di Palo, Edward Johns focuses on how Transformers, including GPT-4 Turbo, can perform few-shot in-context visual imitation learning. \\n\\nThis paper discusses repurposing natural language models for embodied tasks, including utilizing Transformers in the vision and action domains to learn general patterns in demonstration data for highly efficient imitation learning.\\n\\nNow, I\\'ll provide a Python script to pull the paper text from the arXiv link. However, please note that because arXiv papers are typically in PDF format, this script might not be able to successfully extract the text. If the script fails, we recommend manually downloading and reading the PDF for information specific to software applications.\\n\\nLet\\'s execute the following Python code:\\n\\n```python\\n# Python code\\nimport requests\\nfrom bs4 import BeautifulSoup\\nimport re\\n\\ndef extract_content(url):\\n    response = requests.get(url)\\n    html = response.content\\n    soup = BeautifulSoup(html, \"html.parser\")\\n    text = soup.get_text()\\n    return text\\n\\ndef find_applications(text, keywords):\\n    lines = text.split(\\'\\\\n\\')\\n    applications = []\\n    for line in lines:\\n        if any(keyword in line for keyword in keywords):\\n            applications.append(line.strip())\\n    return applications\\n\\npaper_url = \\'http://arxiv.org/abs/2403.19578v1\\'  # Paper link\\nkeywords = [\\'software\\', \\'application\\', \\'development\\', \\'implementation\\', \\'coding\\', \\'programming\\']\\n\\npaper_content = extract_content(paper_url)\\napplications = find_applications(paper_content, keywords)\\n\\nfor application in applications:\\n    print(application)\\n```\\n\\nThis script attempts to scrape the paper content from the provided URL, then search the text for lines containing the keywords we\\'re interested in (software-related terms). It then prints any matching lines.\\n\\nPlease run this script. If it\\'s not able to extract valuable information, you may need to manually read the paper by clicking [here](http://arxiv.org/abs/2403.19578v1).', cost=({'total_cost': 0.4014, 'gpt-4': {'cost': 0.4014, 'prompt_tokens': 13346, 'completion_tokens': 17, 'total_tokens': 13363}}, {'total_cost': 0.30185999999999996, 'gpt-4': {'cost': 0.30185999999999996, 'prompt_tokens': 10044, 'completion_tokens': 9, 'total_tokens': 10053}}), human_input=[])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "user_proxy.initiate_chat(\n",
    "    manager, message=\"Find a latest paper about gpt-4 on arxiv and find its potential applications in software.\"\n",
    ")\n",
    "# type exit to terminate the chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "front_matter": {
   "description": "Explore the utilization of large language models in automated group chat scenarios, where agents perform tasks collectively, demonstrating how they can be configured, interact with each other, and retrieve specific information from external resources.",
   "tags": [
    "orchestration",
    "group chat"
   ]
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
